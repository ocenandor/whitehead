{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from itertools import islice\n",
    "from os import environ\n",
    "from pickle import dump\n",
    "\n",
    "\n",
    "from freegroup.sampling import freegroup\n",
    "from freegroup.sampling.helper import get_rng\n",
    "from freegroup.tools import Comm, flatten, to_string\n",
    "from iteration_utilities import repeatfunc, unique_everseen\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tokenizer import build_tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "rng = get_rng(seed)\n",
    "\n",
    "fdim = 2\n",
    "num_pairs = 2\n",
    "\n",
    "L = 50\n",
    "# train dataset size\n",
    "N = int(1e2)\n",
    "\n",
    "tokenizer = build_tokenizer(\n",
    "    \"word-level\",\n",
    "    fdim=fdim,\n",
    "    add_commutator_tokens=False,\n",
    "    add_prompt_tokens=True,\n",
    "    add_post_processor=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from freegroup.sampling import CFGNormalClosureSampler\n",
    "\n",
    "\n",
    "def generate_random_closure(fdim, max_length=5):\n",
    "    # TODO: avoid trivial closures, maybe freegroup has smth for this?\n",
    "    length = rng.integers(1, max_length)\n",
    "    closure = []\n",
    "    for _ in range(length):\n",
    "        letter = 0\n",
    "        while letter == 0:\n",
    "            letter = rng.integers(-fdim, fdim)\n",
    "        closure.append(letter)\n",
    "\n",
    "    return closure\n",
    "\n",
    "\n",
    "def generate_closure_pairs(fdim, num_pairs=5):\n",
    "    return [\n",
    "        (generate_random_closure(fdim), generate_random_closure(fdim)) for _ in range(num_pairs)\n",
    "    ]\n",
    "\n",
    "\n",
    "def create_samplers(closure_pairs, fdim):\n",
    "    samplers = []\n",
    "    for r, s in closure_pairs:\n",
    "        R_sampler = CFGNormalClosureSampler.build(closure=r, fdim=fdim)\n",
    "        S_sampler = CFGNormalClosureSampler.build(closure=s, fdim=fdim)\n",
    "        samplers.append((R_sampler, S_sampler))\n",
    "\n",
    "    return samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "closure_pairs = closure_pairs = [\n",
    "    [[-1, 2, 2, 1, -2, -2, -2], [-2, 1, 1, 2, -1, -1, -1]],\n",
    "    [[-1, 2, 1, -2, -2], [-1, -1, -2, 1, 1, 2]]\n",
    "]\n",
    "samplers = create_samplers(closure_pairs, fdim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_whitehead_multilabel(label, num_pairs):\n",
    "    # num_pairs = len(closure_pairs)\n",
    "    if label.startswith(\"r\"):\n",
    "        return [int(label[1:])]\n",
    "    elif label.startswith(\"s\"):\n",
    "        return [num_pairs + int(label[1:])]\n",
    "    elif label == \"f\":\n",
    "        return []\n",
    "    elif label == \"c\":\n",
    "        return list(range(2 * num_pairs))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(n_samples, rng, sampler, label):\n",
    "    def fn():\n",
    "        length = rng.integers(1, L + 1)\n",
    "        try:\n",
    "            word = sampler(length=length, rng=rng)\n",
    "            return {\n",
    "                \"label\": label,\n",
    "                \"multilabel\": get_whitehead_multilabel(label, 2),\n",
    "                \"word_str\": to_string(word),\n",
    "            }\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    iterator = repeatfunc(fn)\n",
    "    iterator = filter(lambda x: x is not None, iterator)\n",
    "    iterator = unique_everseen(iterator)\n",
    "    iterator = islice(iterator, n_samples)\n",
    "\n",
    "    return list(tqdm(iterator, total=int(n_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samplers[0][0](length=rng.integers(1, L + 1), rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_freegroup(n_samples=1e3, rng=rng, label=\"f\"):\n",
    "    def fn():\n",
    "        length = rng.integers(1, L + 1)\n",
    "        word = freegroup(2, length, rng=rng)\n",
    "        return {\n",
    "            \"label\": label,\n",
    "            \"multilabel\": get_whitehead_multilabel(label, 2),  # if coin else 's',\n",
    "            \"word_str\": to_string(word),\n",
    "        }\n",
    "\n",
    "    iterator = repeatfunc(fn)\n",
    "    iterator = unique_everseen(iterator)\n",
    "    iterator = islice(iterator, n_samples)\n",
    "\n",
    "    return list(tqdm(iterator, total=int(n_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_comm(n_samples=1e3, rng=rng, samplers=None, label=\"c\"):\n",
    "    def fn():\n",
    "        words = []\n",
    "        for R_sampler, S_sampler in samplers:\n",
    "            for sampler in [R_sampler, S_sampler]:\n",
    "                flag = False\n",
    "                while not flag:\n",
    "                    length = rng.integers(1, L // (5 * len(samplers)) + 1)\n",
    "                    try:\n",
    "                        word = sampler(length=length, rng=rng)\n",
    "                        words.append(word)\n",
    "                        flag = True\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "        i, j = rng.choice(len(words), size=2, replace=False)\n",
    "        word1, word2 = words[i], words[j]\n",
    "\n",
    "        coin = rng.integers(low=0, high=2)\n",
    "        if coin:\n",
    "            result = flatten(Comm([word1, word2]))\n",
    "        else:\n",
    "            result = flatten(Comm([word2, word1]))\n",
    "\n",
    "        return {\n",
    "            \"label\": label,\n",
    "            \"multilabel\": get_whitehead_multilabel(label, 2),\n",
    "            \"word_str\": to_string(result),\n",
    "        }\n",
    "\n",
    "    iterator = repeatfunc(fn)\n",
    "    iterator = unique_everseen(iterator)\n",
    "    iterator = islice(iterator, n_samples)\n",
    "\n",
    "    return list(tqdm(iterator, total=int(n_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 500.42it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 914.89it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 941.36it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 900.65it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 2741.59it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "for i, (R, S) in enumerate(samplers):\n",
    "    dataset += sample(N // (2 * num_pairs), rng, R, f\"r{i}\")\n",
    "    dataset += sample(N // (2 * num_pairs), rng, S, f\"s{i}\")\n",
    "\n",
    "dataset += sample_freegroup(N // 2)\n",
    "dataset += sample_comm(N // 2, samplers=samplers)\n",
    "\n",
    "train, test = train_test_split(deepcopy(dataset), test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/main/draft-v2/pavel-tikhomirov-runs/fdim-2-whitehead:v0/train.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'c',\n",
       " 'multilabel': [0, 1, 2],\n",
       " 'word_str': '2 2 1 1 1 1 1 1 -2 -2 -1 -2 -1 2 1 1 2 2 -1 -1 -1 -1 -1 -1 -2 -2 -1 -1 -2 1 2 1'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import to_tensor\n",
    "\n",
    "\n",
    "def train_collate_fn(batch, tokenizer, fdim, num_pairs):\n",
    "    words = list(map(lambda x: x[\"word_str\"], batch))\n",
    "    multilabels = list(map(lambda x: x[\"multilabel\"], batch))\n",
    "\n",
    "    batch = to_tensor(\n",
    "        words, tokenizer, padding=True, prompt_multilabels=multilabels, prompt_strategy_fdim=fdim\n",
    "    )\n",
    "\n",
    "    print(batch)\n",
    "    batch[\"labels\"] = batch[\"input_ids\"].clone()\n",
    "    batch[\"input_ids\"] = batch[\"input_ids\"]\n",
    "    batch[\"attention_mask\"] = batch[\"attention_mask\"]\n",
    "\n",
    "    # Avoid predicting <pad>\n",
    "    batch[\"labels\"][batch[\"attention_mask\"] == 0] = -100\n",
    "    # Avoid predicting prompt\n",
    "    prompt_size = 1 + fdim + 1 + 2 * num_pairs  # Start + fdim + delimiter + 2 * number of pairs\n",
    "    batch[\"labels\"][:, 1:prompt_size] = -100\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataLoader(\n",
    "    train,\n",
    "    16,\n",
    "    collate_fn=partial(train_collate_fn, tokenizer=tokenizer, fdim=fdim, num_pairs=num_pairs),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
