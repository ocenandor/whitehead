{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Генерируем соотношения разной длины\n",
    "* Генерируем слова разной длины из данного соотношения (замеряем время)\n",
    "* Генерируем слова разной длины из свободной группы (замеряем время)\n",
    "* Прогоняем метрики на том и на другом (замеряем время)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from itertools import islice\n",
    "from os import environ\n",
    "from pickle import dump\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "from iteration_utilities import repeatfunc, unique_everseen\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tokenizer import build_tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from utils import train_collate_fn\n",
    "\n",
    "from freegroup.derivatives import max_gamma_contains\n",
    "from freegroup.sampling import CFGNormalClosureSampler, freegroup\n",
    "from freegroup.sampling.helper import get_rng\n",
    "from freegroup.tools import (Comm, batch_magnus_is_from_normal_closure,\n",
    "                             batch_magnus_reduce_modulo_normal_closure,\n",
    "                             flatten, magnus_is_from_normal_closure,\n",
    "                             magnus_reduce_modulo_normal_closure, to_string)\n",
    "\n",
    "sys.path.append('..')\n",
    "from lming.metrics import batch_max_gamma_contains_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Длина соотношений [2, 50] - по пять штук\n",
    "* Длина слов до 200\n",
    "* Ограничение по времени / количеству сэмплов для генерации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "rng = get_rng(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdim  = 2\n",
    "\n",
    "max_relation_length = 20\n",
    "n_relation_per_length = 3\n",
    "time_limit_relation = 5 # to generate n_relation_per_length words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to calculate relations <= 90 sec\n"
     ]
    }
   ],
   "source": [
    "print('time to calculate relations <=', len(range(2, max_relation_length)) * time_limit_relation, 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_freegroup(sample_length, n_samples, fdim=2, time_limit=10, rng=rng):\n",
    "    start = time.time()\n",
    "    words = set()\n",
    "    while time.time() - start < time_limit and len(words) < n_samples:\n",
    "        words.add(tuple(freegroup(fdim, sample_length, rng=rng)))\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d29a31e8db484aac282b6246a99c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "relations = {}\n",
    "for relation_length in tqdm(range(2, max_relation_length, 3)):\n",
    "    relations.setdefault(relation_length, dict())\n",
    "    relations_set = sample_freegroup(relation_length, n_relation_per_length)\n",
    "    for relation in relations_set:\n",
    "        relations[relation_length][relation] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_relation(sample_length, n_samples, sampler, time_limit=10, rng=rng):\n",
    "    SIM_LEN_TIME = .1\n",
    "    start = time.time()\n",
    "    last_new_word_time = time.time()\n",
    "    words = set()\n",
    "    cur_length = len(words)\n",
    "    while time.time() - start < time_limit and len(words) < n_samples:\n",
    "        try:\n",
    "            word = sampler(length=sample_length, rng=rng)\n",
    "            words.add(tuple(word))\n",
    "            if len(words) != cur_length:\n",
    "                last_new_word_time = time.time()\n",
    "                cur_length = len(words)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "        if len(words) == cur_length and time.time() - last_new_word_time > SIM_LEN_TIME:\n",
    "            return words\n",
    "        \n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_word_length = 50\n",
    "n_words_per_length = 50\n",
    "time_limit_word = 1 # to generate n_words_per_length words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to generate words per relation <= 50 sec\n",
      "time to generate words overall <= 900 sec\n"
     ]
    }
   ],
   "source": [
    "time_per_relation = time_limit_word * max_word_length\n",
    "print('time to generate words per relation <=', time_per_relation, 'sec')\n",
    "print('time to generate words overall <=', time_per_relation * len(relations) * n_relation_per_length, 'sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samplers = {key: [] for key in relations}\n",
    "# samplers_time = {key: [] for key in relations} \n",
    "\n",
    "# for relation_length, relation_dict in tqdm(relations.items()):\n",
    "#     for relation in relation_dict:\n",
    "#         # Create sampler\n",
    "#         sampler_start = time.time()\n",
    "#         sampler = CFGNormalClosureSampler.build(closure=list(relation),\n",
    "#                                                 fdim=fdim,\n",
    "#                                                 max_length=max_word_length)\n",
    "#         sampler_end = time.time()\n",
    "#         samplers_time[relation_length].append(sampler_end - sampler_start)\n",
    "#         samplers[relation_length].append(sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ОТДЕЛЬНО ПОСЧИТАТЬА ДЛЯ СЛОВ ИЗ СВОИХ ЗАМЫКАНИЙ И ДЛЯ СВОБОДНОЙ ГРУППЫ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Для замыканий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for relation_length, relation_dict in tqdm(relations.items()):\n",
    "#     for relation, sampler in zip(relation_dict.keys(), samplers[relation_length]):\n",
    "#         words = {}\n",
    "#         # Здесь надо насэмплить слова\n",
    "#         for word_length in range(2, max_word_length):\n",
    "#             words[word_length] = sample_relation(word_length, n_words_per_length, sampler, time_limit=time_limit_word)\n",
    "\n",
    "\n",
    "\n",
    "#         cr_table = np.empty((max_word_length)) * np.nan\n",
    "#         mg_table = np.empty((max_word_length)) * np.nan\n",
    "\n",
    "#         # CALCULATE IS_FROM_NORMAL_CLOSURE\n",
    "#         for word_length, word_set in words.items():\n",
    "#             if word_set:\n",
    "#                 start = time.time()\n",
    "#                 for word in word_set:\n",
    "#                     tmp = magnus_is_from_normal_closure(word, relation)\n",
    "#                 end = time.time()\n",
    "\n",
    "                \n",
    "#                 cr_table[word_length] = (end - start) / len(word_set)\n",
    "\n",
    "#         # CALCULATE MAX_GAMMA_CONTAINS\n",
    "#         for word_length, word_set in words.items():\n",
    "#             if word_set:\n",
    "#                 start = time.time()\n",
    "#                 for word in word_set:\n",
    "#                     tmp = max_gamma_contains(list(word), fdim=fdim)\n",
    "#                 end = time.time()\n",
    "\n",
    "                \n",
    "#                 mg_table[word_length] = (end - start) / len(word_set)        \n",
    "\n",
    "#         # Потом посчитать метрики в среднем по длине слова    \n",
    "\n",
    "#         relations[relation_length][relation] = np.array([cr_table, mg_table])\n",
    "#         break\n",
    "#     break\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "#         # relations[relation_length][relation] = столбик размером с количество длин слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Для свободной группы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba1d3a198c514ac598939a388f91627d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ed0ae720784385ad3cd7e90588ffc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5435c5a682314549ba4c10868e2a261e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df24053db8f1440082521c84609f794b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f6786913123442285fc085f641ab325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for relation_length, relation_dict in tqdm(relations.items()):\n",
    "    for relation in tqdm(relation_dict.keys(), leave=False):\n",
    "        words = {}\n",
    "        # Здесь надо насэмплить слова\n",
    "        for word_length in range(2, max_word_length, 3):\n",
    "            words[word_length] = sample_freegroup(word_length, n_words_per_length, time_limit=time_limit_word)\n",
    "\n",
    "\n",
    "\n",
    "        cr_table = np.empty((max_word_length)) * np.nan\n",
    "        mg_table = np.empty((max_word_length)) * np.nan\n",
    "\n",
    "        # CALCULATE IS_FROM_NORMAL_CLOSURE\n",
    "        for word_length, word_set in words.items():\n",
    "            if word_set:\n",
    "                start = time.time()\n",
    "                for word in word_set:\n",
    "                    tmp = magnus_is_from_normal_closure(word, relation)\n",
    "                end = time.time()\n",
    "\n",
    "                \n",
    "                cr_table[word_length] = (end - start) / len(word_set)\n",
    "\n",
    "        # CALCULATE MAX_GAMMA_CONTAINS\n",
    "        for word_length, word_set in words.items():\n",
    "            if word_set:\n",
    "                start = time.time()\n",
    "                for word in word_set:\n",
    "                    tmp = max_gamma_contains(list(word), fdim=fdim)\n",
    "                end = time.time()\n",
    "\n",
    "                \n",
    "                mg_table[word_length] = (end - start) / len(word_set)        \n",
    "\n",
    "        # Потом посчитать метрики в среднем по длине слова    \n",
    "\n",
    "        relations[relation_length][relation] = np.array([cr_table, mg_table])\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        # relations[relation_length][relation] = столбик размером с количество длин слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmagnus_is_from_normal_closure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/freegroup-0.0.1-py3.10-linux-x86_64.egg/freegroup/tools/word_problem.py:225\u001b[0m, in \u001b[0;36mmagnus_is_from_normal_closure\u001b[0;34m(word, relator, T)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmagnus_is_from_normal_closure\u001b[39m(word, relator, T\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    224\u001b[0m     T \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m() \u001b[38;5;28;01mif\u001b[39;00m T \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mset\u001b[39m([\u001b[38;5;28mabs\u001b[39m(LetterWithSubscript(x)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m T])\n\u001b[0;32m--> 225\u001b[0m     w \u001b[38;5;241m=\u001b[39m \u001b[43mmagnus_reduce_modulo_normal_closure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m w \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mall\u001b[39m([x \u001b[38;5;129;01min\u001b[39;00m T \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m w])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/freegroup-0.0.1-py3.10-linux-x86_64.egg/freegroup/tools/word_problem.py:234\u001b[0m, in \u001b[0;36mmagnus_reduce_modulo_normal_closure\u001b[0;34m(word, relator, T)\u001b[0m\n\u001b[1;32m    231\u001b[0m relator \u001b[38;5;241m=\u001b[39m [LetterWithSubscript(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m relator]\n\u001b[1;32m    232\u001b[0m T \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m() \u001b[38;5;28;01mif\u001b[39;00m T \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mset\u001b[39m([\u001b[38;5;28mabs\u001b[39m(LetterWithSubscript(x)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m T])\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl_reduce_word_problem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/freegroup-0.0.1-py3.10-linux-x86_64.egg/freegroup/tools/word_problem.py:410\u001b[0m, in \u001b[0;36mimpl_reduce_word_problem\u001b[0;34m(word, relator, T)\u001b[0m\n\u001b[1;32m    406\u001b[0m w_prime \u001b[38;5;241m=\u001b[39m psi(word, t, x, alpha, beta)\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m#print('\\t' * #current_depth +f'psi(r)={r_prime}, psi(w)={w_prime}')\u001b[39;00m\n\u001b[0;32m--> 410\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mimpl_reduce_word_problem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw_prime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_prime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m T:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;66;03m# x not in result and t not in result, so no need for inverse\u001b[39;00m\n\u001b[1;32m    413\u001b[0m     result \u001b[38;5;241m=\u001b[39m normalize(result)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/freegroup-0.0.1-py3.10-linux-x86_64.egg/freegroup/tools/word_problem.py:343\u001b[0m, in \u001b[0;36mimpl_reduce_word_problem\u001b[0;34m(word, relator, T)\u001b[0m\n\u001b[1;32m    341\u001b[0m cumsums_, splits_ \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (cumsum, split) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(cumsums, splits)):\n\u001b[0;32m--> 343\u001b[0m     carry_, w \u001b[38;5;241m=\u001b[39m \u001b[43mpropogate_t\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcarry\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcumsum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_prime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m splits_ \u001b[38;5;129;01mand\u001b[39;00m carry \u001b[38;5;241m+\u001b[39m cumsum \u001b[38;5;241m==\u001b[39m carry_:\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;66;03m#print('\\t' * #current_depth +f\"Propogated all t's, so merging to splits\")\u001b[39;00m\n\u001b[1;32m    347\u001b[0m         splits_[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mextend(w)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/freegroup-0.0.1-py3.10-linux-x86_64.egg/freegroup/tools/word_problem.py:170\u001b[0m, in \u001b[0;36mpropogate_t\u001b[0;34m(cumsum, word, relator, t, x, a, b, cache)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m cumsum \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m([v\u001b[38;5;241m.\u001b[39mabs \u001b[38;5;241m==\u001b[39m x_a \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m word]):\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m#print('\\t' * #current_depth +f\"Falling into rewriting x_a with elements from A\")\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(word) \u001b[38;5;129;01min\u001b[39;00m cache[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m--> 170\u001b[0m         cache[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;28mtuple\u001b[39m(word)] \u001b[38;5;241m=\u001b[39m \u001b[43mimpl_reduce_word_problem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     word_prime \u001b[38;5;241m=\u001b[39m cache[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;28mtuple\u001b[39m(word)]\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m#print('\\t' * #current_depth +f\"Got w'= {word_prime}\")\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/freegroup-0.0.1-py3.10-linux-x86_64.egg/freegroup/tools/word_problem.py:410\u001b[0m, in \u001b[0;36mimpl_reduce_word_problem\u001b[0;34m(word, relator, T)\u001b[0m\n\u001b[1;32m    406\u001b[0m w_prime \u001b[38;5;241m=\u001b[39m psi(word, t, x, alpha, beta)\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m#print('\\t' * #current_depth +f'psi(r)={r_prime}, psi(w)={w_prime}')\u001b[39;00m\n\u001b[0;32m--> 410\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mimpl_reduce_word_problem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw_prime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_prime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m T:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;66;03m# x not in result and t not in result, so no need for inverse\u001b[39;00m\n\u001b[1;32m    413\u001b[0m     result \u001b[38;5;241m=\u001b[39m normalize(result)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/freegroup-0.0.1-py3.10-linux-x86_64.egg/freegroup/tools/word_problem.py:382\u001b[0m, in \u001b[0;36mimpl_reduce_word_problem\u001b[0;34m(word, relator, T)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ts, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(cumsums, splits):\n\u001b[1;32m    381\u001b[0m     _add_t_n_times(t, ts, result)\n\u001b[0;32m--> 382\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend(\u001b[43mremove_subscript\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    383\u001b[0m _add_t_n_times(t, cumsums[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], result)\n\u001b[1;32m    384\u001b[0m result \u001b[38;5;241m=\u001b[39m normalize(result)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/freegroup-0.0.1-py3.10-linux-x86_64.egg/freegroup/tools/word_problem.py:125\u001b[0m, in \u001b[0;36mremove_subscript\u001b[0;34m(word, t)\u001b[0m\n\u001b[1;32m    123\u001b[0m w \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m word:\n\u001b[0;32m--> 125\u001b[0m     \u001b[43m_add_t_n_times\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubscript\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     w\u001b[38;5;241m.\u001b[39mappend(v\u001b[38;5;241m.\u001b[39mremove_subscript())\n\u001b[1;32m    127\u001b[0m     _add_t_n_times(\u001b[38;5;241m-\u001b[39mt, v\u001b[38;5;241m.\u001b[39msubscript[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], w)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/freegroup-0.0.1-py3.10-linux-x86_64.egg/freegroup/tools/word_problem.py:118\u001b[0m, in \u001b[0;36m_add_t_n_times\u001b[0;34m(t, n, src)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_add_t_n_times\u001b[39m(t, n, src):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 118\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n): src\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m-\u001b[39mn): src\u001b[38;5;241m.\u001b[39mappend(deepcopy(\u001b[38;5;241m-\u001b[39mt))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/copy.py:148\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    146\u001b[0m     y \u001b[38;5;241m=\u001b[39m copier(x, memo)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43missubclass\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    149\u001b[0m         y \u001b[38;5;241m=\u001b[39m _deepcopy_atomic(x, memo)\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "magnus_is_from_normal_closure(word, relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, -1, 2, 1, -2, 1, 2, -1, 2, 2, 2, 2, -1, -2, 1, 2, -1, -2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, -1, 2, 2, 2, 2, 1, 2, 1, -2, -2, 1, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 2, -1, -2, -1, -1, -1, -1, -1, 2)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'nanmean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrelations\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnanmean\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'nanmean'"
     ]
    }
   ],
   "source": [
    "relations[2][(1,-2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([       nan,        nan, 0.0002627 ,        nan,        nan,\n",
       "       0.00038651,        nan,        nan, 0.00051949,        nan,\n",
       "              nan, 0.00065054,        nan,        nan, 0.00078794,\n",
       "              nan,        nan, 0.00092748,        nan,        nan,\n",
       "       0.00106255,        nan,        nan, 0.00118923,        nan,\n",
       "              nan, 0.0013168 ,        nan,        nan, 0.00146978,\n",
       "              nan,        nan, 0.0016012 ,        nan,        nan,\n",
       "       0.00173427,        nan,        nan, 0.00185146,        nan,\n",
       "              nan, 0.00197187,        nan,        nan, 0.00215027,\n",
       "              nan,        nan, 0.00225279,        nan,        nan])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations[2][(1, -2)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([       nan,        nan, 0.00095208,        nan,        nan,\n",
       "       0.00153477,        nan,        nan, 0.00332066,        nan,\n",
       "              nan, 0.0048939 ,        nan,        nan, 0.00638343,\n",
       "              nan,        nan, 0.00875181,        nan,        nan,\n",
       "       0.01582205,        nan,        nan, 0.01502557,        nan,\n",
       "              nan, 0.01514737,        nan,        nan, 0.02404424,\n",
       "              nan,        nan, 0.02713127,        nan,        nan,\n",
       "       0.03287436,        nan,        nan, 0.03613797,        nan,\n",
       "              nan, 0.04956366,        nan,        nan, 0.04982257,\n",
       "              nan,        nan, 0.05512855,        nan,        nan])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations[11][(2,\n",
    "  1,\n",
    "  1,\n",
    "  1,\n",
    "  2,\n",
    "  -1,\n",
    "  -2,\n",
    "  -1,\n",
    "  2,\n",
    "  -1,\n",
    "  -2)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([(2, 1, 1, 1, 2, -1, -2, -1, 2, -1, -2), (2, 2, 2, -1, -2, -1, -1, -1, -1, -1, 2), (2, 1, 1, 2, -1, -2, -1, -1, -2, 1, 2)])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations[11].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([2, 5, 8, 11, 14, 17])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 2,\n",
       " -1,\n",
       " 2,\n",
       " 1,\n",
       " -2,\n",
       " 1,\n",
       " 2,\n",
       " -1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " -1,\n",
       " -2,\n",
       " 1,\n",
       " 2,\n",
       " -1,\n",
       " -2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " -1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " -2,\n",
       " -2,\n",
       " 1,\n",
       " 2,\n",
       " 2)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['relations_time_experiment.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(relations, 'relations_time_experiment.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "L = 100\n",
    "fdim =2 \n",
    "length = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11026aba1fda4397ba6da1779e4f72f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "words = set()\n",
    "while time.time() - start < time_limit and len(words) < n_samples:\n",
    "    words.add(to_string(freegroup(fdim, length, rng=rng)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2 -1'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{to_string(freegroup(2, 2)), to_string(freegroup(2, 2))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn():\n",
    "    word = freegroup(fdim, length, rng=rng)\n",
    "    return word\n",
    "\n",
    "iterator = repeatfunc(fn)\n",
    "iterator = unique_everseen(iterator)\n",
    "# iterator = islice(iterator, 100)\n",
    "words = []\n",
    "for el in iterator:\n",
    "    words.append(el)\n",
    "\n",
    "# return list(tqdm(iterator, total=int(n_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_freegroup(fdim=2, length=2, n_samples=int(1e3), max_time=100, rng=rng): \n",
    "    # функция асэмплит слов с определенными fdim и length\n",
    "    # label = 'f'\n",
    "    start = time.start()\n",
    "    def fn():\n",
    "        word = freegroup(fdim, length, rng=rng)\n",
    "        return word\n",
    "            # {\n",
    "            # \"word_str\" : to_string(word),\n",
    "            # \"label\": label,\n",
    "            # }\n",
    "    iterator = repeatfunc(fn)\n",
    "    iterator = unique_everseen(iterator)\n",
    "    iterator = islice(iterator, n_samples)\n",
    "\n",
    "    return list(tqdm(iterator, total=int(n_samples)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
