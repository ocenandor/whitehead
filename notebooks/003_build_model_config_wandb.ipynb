{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a config with tokenizer and model from h-face for use in training and upload it to wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from tokenizer import build_tokenizer\n",
    "from transformers import AutoModelForCausalLM, GPTNeoXConfig\n",
    "from os import environ\n",
    "\n",
    "environ['WANDB_USERNAME']='pavel-tikhomirov'\n",
    "environ['WANDB_DIR']=f'/main/draft-v2/{environ[\"WANDB_USERNAME\"]}-runs/'\n",
    "environ['TOKENIZERS_PARALLELISM']='false'\n",
    "\n",
    "\n",
    "tokenizer = build_tokenizer('word-level', fdim=3, add_commutator_tokens=True,\n",
    "                            add_prompt_tokens=True, add_post_processor=True)\n",
    "\n",
    "\n",
    "from transformers import GPT2Config\n",
    "config = GPT2Config(\n",
    "    vocab_size = len(tokenizer.get_vocab()),\n",
    "    n_embd     = 768,\n",
    "    n_layer    = 8,\n",
    "    n_head     = 6,\n",
    "    n_inner    = 512,\n",
    "    \n",
    "    bos_token_id = tokenizer.bos_token_id,\n",
    "    eos_token_id = tokenizer.eos_token_id,\n",
    "    pad_token_id = tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpavel-tikhomirov\u001b[0m (\u001b[33mml-in-algebraic-topology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/main/draft-v2/pavel-tikhomirov-runs/wandb/run-20241116_232206-e6n8fr2q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ml-in-algebraic-topology/whitehead/runs/e6n8fr2q' target=\"_blank\">fiery-gorge-217</a></strong> to <a href='https://wandb.ai/ml-in-algebraic-topology/whitehead' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ml-in-algebraic-topology/whitehead' target=\"_blank\">https://wandb.ai/ml-in-algebraic-topology/whitehead</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ml-in-algebraic-topology/whitehead/runs/e6n8fr2q' target=\"_blank\">https://wandb.ai/ml-in-algebraic-topology/whitehead/runs/e6n8fr2q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/tmp/tmpoc1avurc)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f5b7485ed84d8896f9bd7e60dd83fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fiery-gorge-217</strong> at: <a href='https://wandb.ai/ml-in-algebraic-topology/whitehead/runs/e6n8fr2q' target=\"_blank\">https://wandb.ai/ml-in-algebraic-topology/whitehead/runs/e6n8fr2q</a><br/> View project at: <a href='https://wandb.ai/ml-in-algebraic-topology/whitehead' target=\"_blank\">https://wandb.ai/ml-in-algebraic-topology/whitehead</a><br/>Synced 5 W&B file(s), 0 media file(s), 5 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/main/draft-v2/pavel-tikhomirov-runs/wandb/run-20241116_232206-e6n8fr2q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "with TemporaryDirectory() as dir,\\\n",
    "     wandb.init(project = 'whitehead', entity = 'ml-in-algebraic-topology', job_type = 'build-model-config'):\n",
    "         \n",
    "    tokenizer.save_pretrained(dir)\n",
    "    config.save_pretrained(dir)\n",
    "\n",
    "    artifact = wandb.Artifact(name = 'gpt-2-fdim-3', type = 'model-config', metadata = {\n",
    "        'parameters': sum(p.numel() for p in model.parameters()),\n",
    "        **config.to_dict(),\n",
    "    }, description = \"GPT2 with `word-level`, with prompt tokens. Add eos and bos. With comas and [, ] (2 extra). Whitehead testing\")\n",
    "\n",
    "    artifact.add_dir(dir)\n",
    "\n",
    "    wandb.run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
