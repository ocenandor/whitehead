{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from itertools import islice\n",
    "from os import environ\n",
    "from pickle import dump\n",
    "\n",
    "\n",
    "from freegroup.sampling import freegroup\n",
    "from freegroup.sampling.helper import get_rng\n",
    "from freegroup.tools import Comm, flatten, to_string\n",
    "from iteration_utilities import repeatfunc, unique_everseen\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tokenizer import build_tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "rng = get_rng(seed)\n",
    "\n",
    "fdim = 3\n",
    "num_pairs = 2\n",
    "\n",
    "L = 50\n",
    "# train dataset size\n",
    "N = int(1e2)\n",
    "\n",
    "tokenizer = build_tokenizer(\n",
    "    \"word-level\",\n",
    "    fdim=fdim,\n",
    "    add_commutator_tokens=False,\n",
    "    add_prompt_tokens=True,\n",
    "    add_post_processor=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from freegroup.sampling import freegroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, -2, -1, -1, 3, 3, 2, 2, 1, 3]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freegroup(fdim=3, length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from freegroup.sampling import CFGNormalClosureSampler\n",
    "\n",
    "\n",
    "def generate_random_closure(fdim, max_length=5):\n",
    "    # TODO: avoid trivial closures, maybe freegroup has smth for this?\n",
    "    length = rng.integers(1, max_length)\n",
    "    closure = []\n",
    "    for _ in range(length):\n",
    "        letter = 0\n",
    "        while letter == 0:\n",
    "            letter = rng.integers(-fdim, fdim)\n",
    "        closure.append(letter)\n",
    "\n",
    "    return closure\n",
    "\n",
    "\n",
    "def generate_closure_pairs(fdim, num_pairs=5):\n",
    "    return [\n",
    "        (generate_random_closure(fdim), generate_random_closure(fdim)) for _ in range(num_pairs)\n",
    "    ]\n",
    "\n",
    "\n",
    "def create_samplers(closure_pairs, fdim):\n",
    "    samplers = []\n",
    "    for r, s in closure_pairs:\n",
    "        R_sampler = CFGNormalClosureSampler.build(closure=r, fdim=fdim)\n",
    "        S_sampler = CFGNormalClosureSampler.build(closure=s, fdim=fdim)\n",
    "        samplers.append((R_sampler, S_sampler))\n",
    "\n",
    "    return samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "closure_pairs = generate_closure_pairs(fdim, num_pairs)\n",
    "samplers = create_samplers(closure_pairs, fdim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_whitehead_multilabel(label):\n",
    "    num_pairs = len(closure_pairs)\n",
    "    if label.startswith(\"r\"):\n",
    "        return [int(label[1:])]\n",
    "    elif label.startswith(\"s\"):\n",
    "        return [num_pairs + int(label[1:])]\n",
    "    elif label == \"f\":\n",
    "        return []\n",
    "    elif label == \"c\":\n",
    "        return list(range(2 * num_pairs))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(n_samples, rng, sampler, label):\n",
    "    def fn():\n",
    "        length = rng.integers(1, L + 1)\n",
    "        try:\n",
    "            word = sampler(length=length, rng=rng)\n",
    "            return {\n",
    "                \"label\": label,\n",
    "                \"multilabel\": get_whitehead_multilabel(label),\n",
    "                \"word_str\": to_string(word),\n",
    "            }\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    iterator = repeatfunc(fn)\n",
    "    iterator = filter(lambda x: x is not None, iterator)\n",
    "    iterator = unique_everseen(iterator)\n",
    "    iterator = islice(iterator, n_samples)\n",
    "\n",
    "    return list(tqdm(iterator, total=int(n_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_freegroup(n_samples=1e3, rng=rng, label=\"f\"):\n",
    "    def fn():\n",
    "        length = rng.integers(1, L + 1)\n",
    "        word = freegroup(2, length, rng=rng)\n",
    "        return {\n",
    "            \"label\": label,\n",
    "            \"multilabel\": get_whitehead_multilabel(label),  # if coin else 's',\n",
    "            \"word_str\": to_string(word),\n",
    "        }\n",
    "\n",
    "    iterator = repeatfunc(fn)\n",
    "    iterator = unique_everseen(iterator)\n",
    "    iterator = islice(iterator, n_samples)\n",
    "\n",
    "    return list(tqdm(iterator, total=int(n_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_comm(n_samples=1e3, rng=rng, samplers=None, label=\"c\"):\n",
    "    def fn():\n",
    "        words = []\n",
    "        for R_sampler, S_sampler in samplers:\n",
    "            for sampler in [R_sampler, S_sampler]:\n",
    "                flag = False\n",
    "                while not flag:\n",
    "                    length = rng.integers(1, L // (5 * len(samplers)) + 1)\n",
    "                    try:\n",
    "                        word = sampler(length=length, rng=rng)\n",
    "                        words.append(word)\n",
    "                        flag = True\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "        i, j = rng.choice(len(words), size=2, replace=False)\n",
    "        word1, word2 = words[i], words[j]\n",
    "\n",
    "        coin = rng.integers(low=0, high=2)\n",
    "        if coin:\n",
    "            result = flatten(Comm([word1, word2]))\n",
    "        else:\n",
    "            result = flatten(Comm([word2, word1]))\n",
    "\n",
    "        return {\n",
    "            \"label\": label,\n",
    "            \"multilabel\": get_whitehead_multilabel(label),\n",
    "            \"word_str\": to_string(result),\n",
    "        }\n",
    "\n",
    "    iterator = repeatfunc(fn)\n",
    "    iterator = unique_everseen(iterator)\n",
    "    iterator = islice(iterator, n_samples)\n",
    "\n",
    "    return list(tqdm(iterator, total=int(n_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f6e3b8faf348f98100cf70c60643b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759cec6f403147c19c16610c131e6333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9ccb489a59444182fd76e56419eea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265646e28a12471d94ce573127a33350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8704bdf1693645ee946cc336c05950e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bcb374c07e344c1bfc3345bb0a762bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = []\n",
    "for i, (R, S) in enumerate(samplers):\n",
    "    dataset += sample(N // (2 * num_pairs), rng, R, f\"r{i}\")\n",
    "    dataset += sample(N // (2 * num_pairs), rng, S, f\"s{i}\")\n",
    "\n",
    "dataset += sample_freegroup(N // 2)\n",
    "dataset += sample_comm(N // 2, samplers=samplers)\n",
    "\n",
    "train, test = train_test_split(deepcopy(dataset), test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import to_tensor\n",
    "\n",
    "\n",
    "def train_collate_fn(batch, tokenizer, fdim, num_pairs):\n",
    "    words = list(map(lambda x: x[\"word_str\"], batch))\n",
    "    multilabels = list(map(lambda x: x[\"multilabel\"], batch))\n",
    "\n",
    "    batch = to_tensor(\n",
    "        words, tokenizer, padding=True, prompt_multilabels=multilabels, prompt_strategy_fdim=fdim\n",
    "    )\n",
    "\n",
    "    print(batch)\n",
    "    batch[\"labels\"] = batch[\"input_ids\"].clone()\n",
    "    batch[\"input_ids\"] = batch[\"input_ids\"]\n",
    "    batch[\"attention_mask\"] = batch[\"attention_mask\"]\n",
    "\n",
    "    # Avoid predicting <pad>\n",
    "    batch[\"labels\"][batch[\"attention_mask\"] == 0] = -100\n",
    "    # Avoid predicting prompt\n",
    "    prompt_size = 1 + fdim + 1 + 2 * num_pairs  # Start + fdim + delimiter + 2 * number of pairs\n",
    "    batch[\"labels\"][:, 1:prompt_size] = -100\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataLoader(\n",
    "    train,\n",
    "    16,\n",
    "    collate_fn=partial(train_collate_fn, tokenizer=tokenizer, fdim=fdim, num_pairs=num_pairs),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
